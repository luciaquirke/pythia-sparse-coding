{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 autoencoders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "import pickle\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l1_r8/_9/learned_dicts.pt\"\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l2_r4/_19/learned_dicts.pt\"\n",
    "autoencoder_filename = \"/mnt/ssd-cluster/bigrun0308/tied_residual_l4_r4/_9/learned_dicts.pt\"\n",
    "\n",
    "auto_num = 9 # Selects which specific autoencoder to use\n",
    "all_autoencoders = torch.load(autoencoder_filename)\n",
    "num_dictionaries = len(all_autoencoders)\n",
    "print(f\"Loaded {num_dictionaries} autoencoders\")\n",
    "autoencoder, hyperparams = all_autoencoders[auto_num]\n",
    "l1_alpha = hyperparams['l1_alpha']\n",
    "autoencoder2, hyperparams2 = all_autoencoders[auto_num+1]\n",
    "smaller_dict = autoencoder.get_learned_dict()\n",
    "larger_dict = autoencoder2.get_learned_dict()\n",
    "\n",
    "#Change these settings to load the correct autoencoder\n",
    "layer = 4\n",
    "setting = \"residual\"\n",
    "# setting = \"attention\"\n",
    "# setting = \"mlp\"\n",
    "# setting = \"mlp_out\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# model_name = \"EleutherAI/pythia-160m\"\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "elif setting == \"attention\":\n",
    "    cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp_out\":\n",
    "    cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "    neurons = model.cfg.d_model\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dict_size': 2048, 'l1_alpha': 0.0015848932089284062},\n",
       " {'dict_size': 2048, 'l1_alpha': 0.002154434798285365})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams, hyperparams2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pca = \"/mnt/ssd-cluster/baselines/l4_residual/pca_topk.pt\"\n",
    "pca_topk = torch.load(filename_pca)\n",
    "filename_ica = \"/mnt/ssd-cluster/baselines/l4_residual/ica_topk.pt\"\n",
    "ica_topk = torch.load(filename_ica)\n",
    "pca_dict = pca_topk.get_learned_dict()\n",
    "ica_dict = ica_topk.get_learned_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCS\n",
    "Max cosine similarity between one dictionary & another one. If they learned the same feature, then they'll have high cosine similarity. \n",
    "\n",
    "If two dictionaries learned it, it's probably a real feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# of features above 0.9:', 787)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "#Dictionary Comparison\n",
    "smaller_dict_features, _ = smaller_dict.shape\n",
    "larger_dict_features, _ = larger_dict.shape\n",
    "larger_dict = larger_dict.to(device)\n",
    "# Hungary algorithm\n",
    "# Calculate all cosine similarities and store in a 2D array\n",
    "cos_sims = np.zeros((smaller_dict_features, larger_dict_features))\n",
    "for idx, vector in enumerate(smaller_dict):\n",
    "    cos_sims[idx] = torch.nn.functional.cosine_similarity(vector.to(device), larger_dict, dim=1).cpu().numpy()\n",
    "# Convert to a minimization problem\n",
    "cos_sims = 1 - cos_sims\n",
    "# Use the Hungarian algorithm to solve the assignment problem\n",
    "row_ind, col_ind = linear_sum_assignment(cos_sims)\n",
    "# Retrieve the max cosine similarities and corresponding indices\n",
    "max_cosine_similarities = 1 - cos_sims[row_ind, col_ind]\n",
    "\n",
    "# Get the indices of the max cosine similarities in descending order\n",
    "max_indices = np.argsort(max_cosine_similarities)[::-1]\n",
    "max_cosine_similarities[max_indices][:20]\n",
    "print((\"# of features above 0.9:\", (max_cosine_similarities > .9).sum()))\n",
    "# Plot histogram of max_cosine_similarities\n",
    "plt.hist(max_cosine_similarities, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model activations & Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount= 40\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:09<00:00, 31.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "# neurons = model.W_in.shape[-1]\n",
    "neurons = model.cfg.d_model\n",
    "datapoints = dataset.num_rows\n",
    "batch_size = 32\n",
    "neuron_activations = torch.zeros((datapoints*token_amount, neurons))\n",
    "dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features))\n",
    "pca_dictionary_activations = torch.zeros((datapoints*token_amount, pca_dict.shape[0]))\n",
    "ica_dictionary_activations = torch.zeros((datapoints*token_amount, ica_dict.shape[0]))\n",
    "smaller_auto_encoder = autoencoder\n",
    "smaller_auto_encoder.to_device(device)\n",
    "pca_topk.to_device(device)\n",
    "ica_topk.to_device(device)\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        _, cache = model.run_with_cache(batch.to(device))\n",
    "        batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "        neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "        batched_dictionary_activations = smaller_auto_encoder.encode(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu()\n",
    "        # pca dictionary\n",
    "        batched_pca_dictionary_activations = pca_topk.encode(batched_neuron_activations)\n",
    "        pca_dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_pca_dictionary_activations.cpu()\n",
    "        # ica dictionary\n",
    "        batched_ica_dictionary_activations = ica_topk.encode(batched_neuron_activations)\n",
    "        ica_dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_ica_dictionary_activations.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61.4651)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_activations[:10000].count_nonzero(dim=1).float().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Activation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints(feature_index, dictionary_activations, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    datapoint_indices =[np.unravel_index(i, (datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(model.tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = model.tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list\n",
    "\n",
    "def get_neuron_activation(token, feature, model, autoencoder, setting=\"dictionary_basis\"):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "        neuron_act_batch = cache[cache_name]\n",
    "        if setting==\"dictionary_basis\":\n",
    "            neuron_act_batch = rearrange(neuron_act_batch, \"b s n -> (b s) n\" )\n",
    "            act = autoencoder.encode(neuron_act_batch)\n",
    "            return act[:, feature].tolist()\n",
    "        else: # neuron/residual basis\n",
    "            return neuron_act_batch[0, :, feature].tolist()\n",
    "\n",
    "def ablate_text(text, feature, model, autoencoder, setting=\"dictionary_basis\"):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    display_text_list = []\n",
    "    activation_list = []\n",
    "    for t in text:\n",
    "        # Convert text into tokens\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t equals tokens\n",
    "            tokens = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        seq_size = tokens.shape[1]\n",
    "        if(seq_size == 1): # If the text is a single token, we can't ablate it\n",
    "            continue\n",
    "        original = get_neuron_activation(tokens, feature, model, autoencoder)[-1]\n",
    "        changed_activations = torch.zeros(seq_size, device=device).cpu()\n",
    "        for i in range(seq_size):\n",
    "            # Remove the i'th token from the input\n",
    "            ablated_tokens = torch.cat((tokens[:,:i], tokens[:,i+1:]), dim=1)\n",
    "            changed_activations[i] += get_neuron_activation(ablated_tokens, feature, model,autoencoder, setting)[-1]\n",
    "        changed_activations -= original\n",
    "        display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        activation_list += changed_activations.tolist() + [0.0]\n",
    "    activation_list = torch.tensor(activation_list).reshape(-1,1,1)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=activation_list)\n",
    "\n",
    "def visualize_text(text, feature, model, autoencoder, setting=\"dictionary_basis\", max_activation = None):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    if isinstance(feature, int):\n",
    "        feature = [feature]\n",
    "    display_text_list = []\n",
    "    act_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            token = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t are tokens\n",
    "            token = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        for f in feature:\n",
    "            display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            act_list += get_neuron_activation(token, f, model, autoencoder, setting) + [0.0]\n",
    "    act_list = torch.tensor(act_list).reshape(-1,1,1)\n",
    "    if(max_activation is not None):\n",
    "        act_list = torch.clamp(act_list, max=max_activation)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=act_list)\n",
    "# Ablate the feature direction of the tokens\n",
    "# token_list is a list of tokens, convert to tensor of shape (batch_size, seq_len)\n",
    "from einops import rearrange\n",
    "def ablate_feature_direction(tokens, feature, model, autoencoder):\n",
    "    def mlp_ablation_hook(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "\n",
    "        # Run through the autoencoder\n",
    "        act = autoencoder.encode(int_val)\n",
    "        feature_to_ablate = feature # TODO: bring this out of the function\n",
    "\n",
    "        # Subtract value with feature direction*act_of_feature\n",
    "        dictionary_for_this_autoencoder = autoencoder.get_learned_dict()\n",
    "        feature_direction = torch.outer(act[:, feature_to_ablate].squeeze(), dictionary_for_this_autoencoder[feature_to_ablate].squeeze())\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value -= feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name, \n",
    "            mlp_ablation_hook\n",
    "            )]\n",
    "        )\n",
    "def add_feature_direction(tokens, feature, model, autoencoder, scalar=1.0):\n",
    "    def residual_add_hook(value, hook):\n",
    "        feature_direction = autoencoder.decoder.weight[:, feature].squeeze()\n",
    "        value += scalar*feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name,\n",
    "            residual_add_hook\n",
    "            )]\n",
    "        )\n",
    "def ablate_feature_direction_display(text, features=None, setting=\"true_tokens\", verbose=False):\n",
    "\n",
    "    if features==None:\n",
    "        features = torch.tensor([best_feature])\n",
    "    if isinstance(features, int):\n",
    "        features = torch.tensor([features])\n",
    "    if isinstance(features, list):\n",
    "        features = torch.tensor(features)\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    text_list = []\n",
    "    logit_list = []\n",
    "    for t in text:\n",
    "        tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        with torch.no_grad():\n",
    "            original_logits = model(tokens).log_softmax(-1).cpu()\n",
    "            ablated_logits = ablate_feature_direction(tokens, features, model, smaller_auto_encoder).log_softmax(-1).cpu()\n",
    "        diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "        tokens = tokens.cpu()\n",
    "        if setting == \"true_tokens\":\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            gather_tokens = rearrange(tokens[:,1:], \"b s -> b s 1\") # TODO: verify this is correct\n",
    "            # Gather the logits for the true tokens\n",
    "            diff = rearrange(diff_logits[:, :-1].gather(-1,gather_tokens), \"b s n -> (b s n)\")\n",
    "        elif setting == \"max\":\n",
    "            # Negate the diff_logits to see which tokens have the largest effect on the neuron\n",
    "            val, ind = (-1*diff_logits).max(-1)\n",
    "            diff = rearrange(val[:, :-1], \"b s -> (b s)\")\n",
    "            diff*= -1 # Negate the values gathered\n",
    "            split_text = model.to_str_tokens(ind, prepend_bos=False)\n",
    "            gather_tokens = rearrange(ind[:,1:], \"1 s -> 1 s 1\")\n",
    "        split_text = split_text[1:] # Remove the first token since we're not predicting it\n",
    "        if(verbose):\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            orig = rearrange(original_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            ablated = rearrange(ablated_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            logit_list += orig.tolist() + [0.0]\n",
    "            logit_list += ablated.tolist() + [0.0]\n",
    "        text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        logit_list += diff.tolist() + [0.0]\n",
    "    logit_list = torch.tensor(logit_list).reshape(-1,1,1)\n",
    "    if verbose:\n",
    "        print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
    "    return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
    "def generate_text(input_text, num_tokens, model, autoencoder, feature, temperature=0.7, setting=\"add\", scalar=1.0):\n",
    "    # Convert input text to tokens\n",
    "    input_ids = model.tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        # Generate logits\n",
    "        with torch.no_grad():\n",
    "            if(setting==\"add\"):\n",
    "                logits = add_feature_direction(input_ids, feature, model, autoencoder, scalar=scalar)\n",
    "            else:\n",
    "                logits = model(input_ids)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        predicted_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append predicted token to input_ids\n",
    "        input_ids = torch.cat((input_ids, predicted_token), dim=-1)\n",
    "\n",
    "    # Decode the tokens to text\n",
    "    output_text = model.tokenizer.decode(input_ids[0])\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Logit Lens\n",
    "def logit_lens(model, best_feature, smaller_dict, layer):\n",
    "    with torch.no_grad():\n",
    "        # There are never-used tokens, which have high norm. We want to ignore these.\n",
    "        bad_ind = (model.W_U.norm(dim=0) > 20)\n",
    "        feature_direction = smaller_dict[best_feature].to(device)\n",
    "        # feature_direction = torch.matmul(feature_direction, model.W_out[layer]) # if MLP\n",
    "        logits = torch.matmul(feature_direction, model.W_U).cpu()\n",
    "    # Don't include bad indices\n",
    "    logits[bad_ind] = -1000\n",
    "    topk_values, topk_indices = torch.topk(logits, 20)\n",
    "    top_text = model.to_str_tokens(topk_indices)\n",
    "    print(f\"{top_text}\")\n",
    "    print(topk_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \" I do like a\"\n",
    "split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "token = model.to_tokens(t, prepend_bos=False)\n",
    "_, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "neuron_act_batch = cache[cache_name]\n",
    "_, act = smaller_auto_encoder(neuron_act_batch)\n",
    "v, i = act[0, -1, :].topk(10)\n",
    "\n",
    "print(\"Activations:\",[round(val,2) for val in v.tolist()])\n",
    "print(\"Feature_ids\", i.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interp\n",
    "Investigate the example sentences the activate this feature.\n",
    "\n",
    "Max: show max activating (tokens,contexts)\n",
    "\n",
    "Uniform: Show range of activations from each bin (e.g. sample an example from 1-2, 2-3, etc). \n",
    "[Note: if a feature is monosemantic, then the full range of activations should be that feature, not just max-activating ones]\n",
    "\n",
    "Full_text: shows the full text example\n",
    "\n",
    "Text_list: shows up to the most activating example (try w/ max activating on a couple of examples to see)\n",
    "\n",
    "ablate_text: remove the context one token at a time, and show the decrease/increase in activation of that feature\n",
    "\n",
    "ablate_feature_direction: removes feature direction from model's activation mid-inference, showing the logit diff in the output for every token.\n",
    "\n",
    "logit_lens: show the logit lens for that feature. If matches ablate_feature_direction, then the computation path is through the residual stream, else, it's through future layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index: 21\n",
      "MCS: 0.6457583904266357\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8afc62ba-0e55\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8afc62ba-0e55\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"Size\", \":\", \"\\r\", \"\\\\newline\", \"-\", \" 0\", \".\", \"1\", \"\\r\", \"\\\\newline\", \"-\", \" 0\", \".\", \"1\", \"\\r\", \"\\\\newline\", \"-\", \" 0\", \"\\n\", \"The\", \" present\", \" invention\", \" relates\", \" to\", \" 1\", \",\", \"3\", \",\", \"4\", \"-\", \"ox\", \"ad\", \"iaz\", \"ole\", \" heterocy\", \"clic\", \" carbon\", \" compounds\", \" and\", \" to\", \" their\", \" preparation\", \" and\", \" use\", \".\", \" More\", \" particularly\", \".\", \" the\", \" invention\", \" relates\", \" to\", \" 1\", \",\", \"\\n\", \"Ad\", \"i\", \" &\", \" Gab\", \"by\", \"'s\", \" room\", \" -\", \" Planning\", \"/\", \"In\", \"sp\", \"iration\", \"\\\\newline\", \"\\\\newline\", \"As\", \" many\", \" of\", \" you\", \" know\", \",\", \" we\", \"'re\", \" expecting\", \" our\", \" third\", \" child\", \" (\", \"first\", \" boy\", \"!)\", \" in\", \" late\", \" March\", \".\", \" Since\", \" it\", \" wouldn\", \"\\n\", \"Comparison\", \" of\", \" the\", \" perin\", \"atal\", \" outcomes\", \" after\", \" laparoscopic\", \" my\", \"ome\", \"ctomy\", \" versus\", \" abdominal\", \" my\", \"ome\", \"\\n\", \"The\", \" secular\", \" trend\", \" in\", \" the\", \" incidence\", \" of\", \" hemorrh\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"rest\", \"angular\", \" save\", \" ignores\", \" changes\", \" to\", \" rest\", \"angular\", \" object\", \"\\\\newline\", \"\\\\newline\", \"I\", \"'m\", \" trying\", \" to\", \" call\", \" save\", \" on\", \" a\", \" rest\", \"angular\", \"ized\", \" object\", \",\", \" but\", \" the\", \" save\", \" method\", \" is\", \" completely\", \" ignoring\", \" any\", \" changes\", \" made\", \" to\", \" the\", \"\\n\", \"Ben\", \"ign\", \" essential\", \" ble\", \"ph\", \"ar\", \"osp\", \"asm\", \" (\", \"BE\", \"B\", \")\", \" and\", \" sp\", \"asmod\", \"ic\", \" tort\", \"ic\", \"oll\", \"is\", \" (\", \"ST\", \")\", \" are\", \" progressive\", \"\\n\", \"While\", \" we\", \" regret\", \" to\", \" inform\", \" you\", \" that\", \" Care\", \"Pages\", \" will\", \" be\", \" shutting\", \" down\", \",\", \" due\", \" to\", \" an\", \" overwhelming\", \" response\", \" from\", \" our\", \" members\", \" seeking\", \" additional\", \" account\", \" support\", \" we\", \" will\", \" be\", \" extending\", \"\\n\", \"V\", \"ANC\", \"OU\", \"VER\", \"\\u2013\", \"He\", \" sal\", \"uted\", \" his\", \" supporters\", \" crowded\", \" in\", \" a\", \" Vancouver\", \" courtroom\", \" to\", \" see\", \" him\", \" off\", \",\", \" told\", \" his\", \" wife\", \" he\", \" loved\", \" her\", \" and\", \" then\", \" the\", \" self\", \"\\n\", \"A\", \" British\", \" t\", \"abl\", \"oid\", \" made\", \" an\", \" embarrassing\", \" error\", \" Friday\", \",\", \" writing\", \" a\", \" hyster\", \"ical\", \" piece\", \" that\", \" incorrectly\", \" reported\", \" the\", \" Defence\", \" Ministry\", \" had\", \" paid\", \" hundreds\", \" of\", \" millions\", \"\\n\", \"St\", \"rat\", \"as\", \"ys\", \" has\", \" dropped\", \" a\", \" full\", \"-\", \"color\", \" 3\", \"D\", \" printed\", \" bombs\", \"hell\", \" with\", \" the\", \" introduction\", \" of\", \" the\", \" new\", \" J\", \"55\", \" 3\", \"D\", \" Pr\", \"inter\", \" that\", \" combines\", \" a\", \" number\", \"\\n\"], \"activations\": [[[-1.532935380935669]], [[-3.357819080352783]], [[-1.8709697723388672]], [[-2.975639820098877]], [[-2.039459466934204]], [[2.236370801925659]], [[1.0325548648834229]], [[-0.483794629573822]], [[-2.6909706592559814]], [[-3.1608002185821533]], [[2.387193202972412]], [[15.057243347167969]], [[3.6851043701171875]], [[7.035012722015381]], [[-2.1181516647338867]], [[-1.7357392311096191]], [[2.7953779697418213]], [[19.71312141418457]], [[0.0]], [[-0.9033321142196655]], [[-0.8214792609214783]], [[1.308894157409668]], [[-0.5064408779144287]], [[-2.479580879211426]], [[0.29869771003723145]], [[1.8305833339691162]], [[-2.7772581577301025]], [[2.7304539680480957]], [[-4.377381324768066]], [[1.2112261056900024]], [[-1.1831406354904175]], [[-2.2381060123443604]], [[0.27673688530921936]], [[-2.481276035308838]], [[1.7937819957733154]], [[-1.1898670196533203]], [[-3.2519326210021973]], [[-0.4289090931415558]], [[-4.317046165466309]], [[-2.0358834266662598]], [[-1.6593996286392212]], [[-4.387216567993164]], [[-1.5475177764892578]], [[-3.5407299995422363]], [[-3.41605806350708]], [[-0.5855416059494019]], [[-0.25562599301338196]], [[0.8307982683181763]], [[2.49299955368042]], [[2.2722318172454834]], [[-0.23342067003250122]], [[-1.9001713991165161]], [[5.467082977294922]], [[14.726360321044922]], [[0.0]], [[-0.24580222368240356]], [[-2.8870551586151123]], [[-3.9175028800964355]], [[0.7085387706756592]], [[-1.8275781869888306]], [[-1.445839762687683]], [[-1.5235824584960938]], [[-3.958012819290161]], [[-3.821310043334961]], [[-4.1507792472839355]], [[-4.27975606918335]], [[-1.366182804107666]], [[-2.819281578063965]], [[-3.9970197677612305]], [[-3.6133954524993896]], [[-2.9113433361053467]], [[-2.4756767749786377]], [[-2.5217013359069824]], [[-0.23049966990947723]], [[1.0074796676635742]], [[-2.4421186447143555]], [[-0.7415396571159363]], [[-1.0633931159973145]], [[-2.1766178607940674]], [[-0.32686665654182434]], [[-1.8603389263153076]], [[-2.4674131870269775]], [[-2.651747703552246]], [[-1.8172770738601685]], [[-1.331961750984192]], [[-1.0130352973937988]], [[-3.061513900756836]], [[-2.6051716804504395]], [[-1.7840285301208496]], [[-2.770869016647339]], [[-1.3354204893112183]], [[-0.9920307397842407]], [[10.185336112976074]], [[0.0]], [[-1.1412250995635986]], [[-2.273265838623047]], [[-0.8672753572463989]], [[-1.9410297870635986]], [[-0.4506980776786804]], [[-2.3973946571350098]], [[-2.4341866970062256]], [[0.40591779351234436]], [[1.3764336109161377]], [[1.4415825605392456]], [[0.16220462322235107]], [[-3.049593687057495]], [[-1.0630767345428467]], [[4.778456211090088]], [[9.178651809692383]], [[0.0]], [[-0.9033306837081909]], [[-3.1640114784240723]], [[-2.7454452514648438]], [[-2.3425490856170654]], [[-0.991929292678833]], [[-3.517256736755371]], [[-1.7795829772949219]], [[3.68025803565979]], [[0.0]], [[-2.2018232345581055]], [[-1.1793022155761719]], [[-2.9979805946350098]], [[0.8446650505065918]], [[-1.0475516319274902]], [[-0.24134045839309692]], [[-1.950310230255127]], [[-2.0059123039245605]], [[-1.6214237213134766]], [[-2.3381083011627197]], [[-0.3327494263648987]], [[-0.5961800217628479]], [[-1.9588963985443115]], [[0.4889828860759735]], [[0.545979380607605]], [[0.8002780675888062]], [[1.4207481145858765]], [[2.888634204864502]], [[-0.5337756276130676]], [[-0.8394444584846497]], [[-0.40874767303466797]], [[-2.611274003982544]], [[0.4853108823299408]], [[0.6450731158256531]], [[-0.080716073513031]], [[-0.37665218114852905]], [[-0.18561315536499023]], [[-2.0393056869506836]], [[-0.838815450668335]], [[-0.24598807096481323]], [[-1.1724989414215088]], [[0.6383855938911438]], [[0.6211546659469604]], [[0.6381930112838745]], [[-1.944312572479248]], [[0.49811556935310364]], [[-1.4586608409881592]], [[-3.488651990890503]], [[-1.5732018947601318]], [[2.152902603149414]], [[0.0]], [[-0.7150623202323914]], [[-1.9078638553619385]], [[-3.7913289070129395]], [[0.5063832998275757]], [[-3.256791591644287]], [[-2.2272701263427734]], [[-0.5963494777679443]], [[-2.0919058322906494]], [[-2.9703099727630615]], [[-0.8887354135513306]], [[-0.08763086795806885]], [[-3.006427049636841]], [[-4.704674243927002]], [[-1.4801582098007202]], [[1.6824191808700562]], [[-2.390950918197632]], [[-1.86403226852417]], [[0.9256442785263062]], [[-0.6463090777397156]], [[-2.5644493103027344]], [[-2.644883871078491]], [[-1.1220479011535645]], [[-2.221508264541626]], [[-2.2898166179656982]], [[-0.9959753751754761]], [[0.0]], [[0.24413788318634033]], [[-1.0426584482192993]], [[-1.8172028064727783]], [[-0.7752601504325867]], [[0.044027090072631836]], [[-0.28574806451797485]], [[-1.0209529399871826]], [[-3.124821186065674]], [[-1.0817824602127075]], [[-0.1440296769142151]], [[-0.8861623406410217]], [[-1.7613639831542969]], [[-0.30694419145584106]], [[-1.4682352542877197]], [[4.678439617156982]], [[-3.687412977218628]], [[-1.1117488145828247]], [[-0.06476646661758423]], [[-0.7899682521820068]], [[-3.475088596343994]], [[-0.771182656288147]], [[-0.8626381158828735]], [[-4.027547359466553]], [[-1.2733713388442993]], [[-2.367096424102783]], [[-0.33791524171829224]], [[0.10297241806983948]], [[-1.8170368671417236]], [[-0.504374623298645]], [[-3.296935796737671]], [[0.0]], [[-1.3918750286102295]], [[-1.0912654399871826]], [[-2.107020616531372]], [[0.22012647986412048]], [[-1.871448278427124]], [[-1.613729476928711]], [[0.5775357484817505]], [[-1.6637072563171387]], [[-0.2561960220336914]], [[-1.5109517574310303]], [[-3.4384326934814453]], [[-2.4684910774230957]], [[-1.671495795249939]], [[-1.4453349113464355]], [[-1.2628540992736816]], [[-2.315032482147217]], [[-2.65143084526062]], [[-0.13167381286621094]], [[-4.234319686889648]], [[-3.936990261077881]], [[-2.2940683364868164]], [[-1.095334768295288]], [[-1.068678855895996]], [[-1.1403794288635254]], [[-1.8708243370056152]], [[1.0523344278335571]], [[-3.4174697399139404]], [[-3.5028724670410156]], [[-1.0960994958877563]], [[-6.592856407165527]], [[0.0]], [[-1.0860247611999512]], [[-1.656486988067627]], [[-1.4495799541473389]], [[1.3529973030090332]], [[-3.5138063430786133]], [[-3.152027130126953]], [[-0.16139137744903564]], [[1.6724474430084229]], [[-3.0904018878936768]], [[-1.5263839960098267]], [[-1.8508293628692627]], [[-3.0587592124938965]], [[-1.3479480743408203]], [[3.6965463161468506]], [[-1.0246784687042236]], [[-6.765268802642822]], [[-1.6411830186843872]], [[-0.36727726459503174]], [[-1.683431625366211]], [[-1.1868915557861328]], [[-2.864532709121704]], [[-0.7926405072212219]], [[-1.1320068836212158]], [[-2.5278120040893555]], [[-8.538981437683105]], [[0.32294100522994995]], [[-9.81903076171875]], [[0.0]], [[-0.10509532690048218]], [[-2.2383475303649902]], [[-2.184608221054077]], [[-1.8733044862747192]], [[-2.4817090034484863]], [[-2.236518383026123]], [[0.14707070589065552]], [[-1.3110995292663574]], [[0.9825177192687988]], [[-2.2071425914764404]], [[-2.5274481773376465]], [[-3.764678716659546]], [[-2.523247241973877]], [[-2.413053035736084]], [[-3.1105234622955322]], [[-3.9356236457824707]], [[-1.9551501274108887]], [[-4.978358745574951]], [[-4.538273334503174]], [[-0.993005096912384]], [[-1.7701303958892822]], [[-0.38495463132858276]], [[-3.1031360626220703]], [[-1.288680911064148]], [[-2.7746310234069824]], [[-0.7023918628692627]], [[-2.495482921600342]], [[-1.522159218788147]], [[-4.111023426055908]], [[-3.7760353088378906]], [[-12.779258728027344]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f579ca9c2e0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 300\n",
    "best_feature = int(max_indices[N])\n",
    "best_feature = 21 # Change this one for global index (N is sorted by MCS)\n",
    "# feature_setting = \"dictionary\"\n",
    "# feature_setting = \"pca\"\n",
    "# feature_setting = \"ica\"\n",
    "feature_setting = \"neuron\"\n",
    "setting=\"dictionary_basis\"\n",
    "if(feature_setting == \"dictionary\"):\n",
    "    autoencoder = smaller_auto_encoder\n",
    "    activations = dictionary_activations\n",
    "elif(feature_setting == \"pca\"):\n",
    "    autoencoder = pca_topk\n",
    "    activations = pca_dictionary_activations\n",
    "elif(feature_setting == \"ica\"):\n",
    "    autoencoder = ica_topk\n",
    "    activations = ica_dictionary_activations\n",
    "elif(feature_setting == \"neuron\"):\n",
    "    autoencoder = smaller_auto_encoder # Doesn't matter actually\n",
    "    activations = neuron_activations\n",
    "    setting = \"neuron_basis\"\n",
    "\n",
    "# autoencoder = pca_topk\n",
    "print(f\"Feature index: {best_feature}\")\n",
    "print(f\"MCS: {max_cosine_similarities[best_feature]}\")\n",
    "text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, activations, dataset, setting=\"uniform\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, activations, dataset, setting=\"max\")\n",
    "visualize_text(text_list, best_feature, model, autoencoder, setting=setting)\n",
    "# visualize_text(full_text, best_feature, model, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-44ba3046-19a7\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-44ba3046-19a7\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"1\", \".\", \" Introduction\", \" {#\", \"sec\", \"0005\", \"\\n\", \"1\", \".\", \" Introduction\", \" {#\", \"s\", \"0005\", \"\\n\", \"1\", \".\", \" Introduction\", \" {#\", \"sec\", \"1\", \"-\", \"cancers\", \"-\", \"12\", \"-\", \"00\", \"185\", \"\\n\", \"Introduction\", \" {#\", \"sec\", \"1\", \"\\n\", \"1\", \".\", \" Introduction\", \" {#\", \"sec\", \"1\", \"-\", \"ijms\", \"-\", \"21\", \"-\", \"039\", \"\\n\", \"![](\", \"br\", \"j\", \"cancer\", \"0001\", \"9\", \"-\", \"01\", \"62\", \".\", \"tif\", \" \\\"\", \"sc\", \"anned\", \"-\", \"page\", \"\\\"){\", \".\", \"482\", \"}\", \"\\\\newline\", \"\\\\newline\", \"![](\", \"br\", \"j\", \"cancer\", \"0001\", \"9\", \"\\n\", \"![](\", \"edin\", \"b\", \"med\", \"j\", \"7\", \"\\n\", \"![](\", \"br\", \"j\", \"cancer\", \"0001\", \"\\n\", \"Introduction\", \"\\\\newline\", \"============\", \"\\\\newline\", \"\\\\newline\", \"B\", \"le\", \"eding\", \" following\", \" cardiac\", \" surgery\", \" requiring\", \" the\", \" use\", \" of\", \" cardi\", \"op\", \"ulmonary\", \" bypass\", \" (\", \"CP\", \"B\", \")\", \" is\", \" associated\", \" with\", \" prolonged\", \" hospital\", \" admission\", \" and\", \" increased\", \" mortality\", \" \\\\[[@\", \"R\", \"1\", \"],[@\", \"R\", \"2\", \"\\n\", \"/*\", \"\\\\newline\", \"\\n\", \"Adv\", \"ances\", \" in\", \" drug\", \" metabolism\", \" screening\", \".\", \"\\\\newline\", \"Develop\", \"ments\", \" in\", \" automation\", \",\", \" analytical\", \" technologies\", \" and\", \" molecular\", \" biology\", \" are\", \" being\", \" exploited\", \" by\", \" drug\", \" metabolism\", \" scientists\", \" in\", \" order\", \" to\", \" provide\", \" enhanced\", \" in\", \" vitro\", \" systems\", \" for\", \"\\n\"], \"activations\": [[[-0.11950397491455078]], [[-0.18931961059570312]], [[0.015408515930175781]], [[-2.8924851417541504]], [[-1.004213809967041]], [[-3.6932077407836914]], [[0.0]], [[-0.17876863479614258]], [[-0.08144855499267578]], [[-0.1379537582397461]], [[-3.0120692253112793]], [[-0.6676158905029297]], [[-3.35660982131958]], [[0.0]], [[-0.1325235366821289]], [[0.0807332992553711]], [[0.009673118591308594]], [[-1.9303736686706543]], [[-0.028667926788330078]], [[0.10307788848876953]], [[-0.009415626525878906]], [[0.2170414924621582]], [[0.10555362701416016]], [[0.4337339401245117]], [[-0.04257774353027344]], [[-0.39752817153930664]], [[-1.6564531326293945]], [[0.0]], [[-0.8378221988677979]], [[-2.55220365524292]], [[-0.5395169258117676]], [[-2.5694217681884766]], [[0.0]], [[-0.08466935157775879]], [[0.12121105194091797]], [[-0.11252999305725098]], [[-1.53741455078125]], [[-0.02412867546081543]], [[0.15251898765563965]], [[0.0029768943786621094]], [[0.4879307746887207]], [[-0.009487628936767578]], [[0.17063546180725098]], [[0.2727975845336914]], [[-1.943312644958496]], [[0.0]], [[0.2920830249786377]], [[0.36397576332092285]], [[0.15307116508483887]], [[0.702458381652832]], [[0.5300006866455078]], [[0.5440983772277832]], [[-0.3940768241882324]], [[0.08297157287597656]], [[0.11948275566101074]], [[0.16129207611083984]], [[0.09995627403259277]], [[0.18103623390197754]], [[0.09866666793823242]], [[0.029863357543945312]], [[0.0731348991394043]], [[0.10805940628051758]], [[-0.1367034912109375]], [[0.081329345703125]], [[0.15328383445739746]], [[0.3442561626434326]], [[-0.30742669105529785]], [[-0.30742669105529785]], [[-1.6187357902526855]], [[0.2585921287536621]], [[0.09185504913330078]], [[0.26552605628967285]], [[0.29637980461120605]], [[-1.463179111480713]], [[0.0]], [[-1.1646933555603027]], [[0.2981739044189453]], [[-0.004502773284912109]], [[-0.0941610336303711]], [[-0.009397029876708984]], [[-1.1646933555603027]], [[0.0]], [[-0.9832515716552734]], [[-0.04535961151123047]], [[0.12585735321044922]], [[-0.12608098983764648]], [[-0.9832515716552734]], [[0.0]], [[-0.12497162818908691]], [[0.0018076896667480469]], [[-0.0384068489074707]], [[-0.06344413757324219]], [[-0.06344413757324219]], [[0.0008406639099121094]], [[0.00736236572265625]], [[-0.0010080337524414062]], [[-0.001436471939086914]], [[0.005442380905151367]], [[-0.0035016536712646484]], [[-0.014368534088134766]], [[0.005570888519287109]], [[-0.013674259185791016]], [[0.011553287506103516]], [[0.03015756607055664]], [[0.005901813507080078]], [[0.03407096862792969]], [[0.02268695831298828]], [[-0.060362815856933594]], [[0.008526802062988281]], [[-0.013574361801147461]], [[0.20518064498901367]], [[-0.031086444854736328]], [[0.058506011962890625]], [[0.010364532470703125]], [[0.018317699432373047]], [[-0.00037384033203125]], [[-0.03507232666015625]], [[-0.072479248046875]], [[0.042348384857177734]], [[0.08297419548034668]], [[-0.3993997573852539]], [[0.10722827911376953]], [[0.16773390769958496]], [[0.755495548248291]], [[0.3825669288635254]], [[-0.3993997573852539]], [[0.0]], [[-0.17589664459228516]], [[-0.17589664459228516]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f556104b280>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_text(text_list, best_feature, model, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-63c0471f-cf12\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-63c0471f-cf12\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\" Trip\", \":\", \" Our\", \" students\", \" travel\", \" to\", \" D\", \".\", \"C\", \".\", \" In\", \"aug\", \"uration\", \"\\\\newline\", \"\\\\newline\", \"Editor\", \"\\u2019\", \"s\", \" note\", \":\", \" Ch\", \"ogh\", \"ri\", \",\", \" F\", \"aire\", \"y\", \" and\", \" O\", \"\\u2019\", \"Brien\", \" are\", \" three\", \" of\", \" four\", \" H\", \"umber\", \" Journal\", \"ism\", \"\\n\", \"bishop\", \" Sean\", \" P\", \".\", \" O\", \"'\", \"Mal\", \"ley\", \",\", \" saying\", \" it\", \" is\", \" time\", \" for\", \" healing\", \" and\", \" reconciliation\", \",\", \" said\", \" yesterday\", \" that\", \" he\", \" will\", \" reconsider\", \" the\", \" Arch\", \"di\", \"ocese\", \" of\", \" Boston\", \"'s\", \" ref\", \"us\", \"als\", \" to\", \" accept\", \" money\", \" raised\", \" by\", \"\\n\", \" content\", \" continued\", \"\\\\newline\", \"\\\\newline\", \"O\", \"\\u2019\", \"Neill\", \" refused\", \" all\", \" comment\", \" when\", \" contacted\", \" by\", \" the\", \" Citizen\", \".\", \" \\u201c\", \"This\", \" is\", \" a\", \" very\", \" personal\", \" situation\", \",\\u201d\", \" said\", \" Patricia\", \" Lynch\", \",\", \" a\", \" spokeswoman\", \" for\", \" the\", \" museum\", \" of\", \" history\", \".\", \" \\u201c\", \"He\", \" doesn\", \"\\n\", \" impossible\", \" thing\", \" suddenly\", \" became\", \"\\\\newline\", \"possible\", \" political\", \" sensation\", \"\\\\newline\", \"bet\", \"o\", \" o\", \"'\", \"r\", \"our\", \"ke\", \" and\", \" never\", \" in\", \" a\", \" million\", \"\\\\newline\", \"years\", \" wanted\", \" to\", \" pursue\", \"\\\\newline\", \"polit\", \"ics\", \" the\", \" tragedy\", \" that\", \" changed\", \"\\\\newline\", \"everything\", \" the\", \" night\", \" before\", \" he\", \"\\n\", \" O\", \"'\", \"D\", \"ow\", \"d\", \" (\", \"S\", \"lig\", \"o\", \" MP\", \")\", \"\\\\newline\", \"\\\\newline\", \"John\", \" O\", \"'\", \"D\", \"ow\", \"d\", \" (\", \"13\", \" February\", \" 18\", \"56\", \" \\u2013\", \" 26\", \" October\", \" 1937\", \")\", \" was\", \" Irish\", \" National\", \"ist\", \" Member\", \" of\", \" Parliament\", \" for\", \" North\", \" S\", \"\\n\", \" UK\", \" Qual\", \"ifications\", \" and\", \" Cur\", \"ric\", \"ulum\", \" Authority\", \" is\", \" considering\", \" introducing\", \" a\", \" new\", \" A\", \"\\u2019\", \"level\", \" course\", \" (\", \"in\", \" Britain\", \",\", \" A\", \"\\u2019\", \"level\", \" is\", \" the\", \" exam\", \" that\", \" is\", \" taken\", \" at\", \" the\", \" end\", \" of\", \" high\", \" school\", \")\", \" called\", \" \\u201c\", \"\\n\", \"Rock\", \"\\u2019\", \"n\", \"\\u2019\", \"Roll\", \" Bangkok\", \"\\u2019\", \" to\", \" be\", \" witnessed\", \" at\", \" The\", \" Over\", \"stay\", \" in\", \" Pink\", \"la\", \"o\", \".\", \" Fe\", \"aturing\", \" five\", \" bands\", \" of\", \" original\", \" and\", \" authentic\", \" R\", \"\\u2019\", \"n\", \"\\u2019\", \"R\", \" music\", \" in\", \" a\", \" most\", \" extravag\", \"ant\", \" and\", \"\\n\", \"ers\", \" fight\", \" closure\", \" of\", \" Bronx\", \" St\", \"ella\", \" D\", \"\\u0092\", \"oro\", \" plant\", \"\\\\newline\", \"\\\\newline\", \"The\", \" union\", \" that\", \" represents\", \" 136\", \" workers\", \" at\", \" the\", \" St\", \"ella\", \" D\", \"\\u0092\", \"oro\", \" B\", \"isc\", \"uit\", \" Co\", \".\", \" has\", \" filed\", \" charges\", \" with\", \" the\", \" National\", \" Labor\", \" Relations\", \"\\n\", \"'\", \"aden\", \" and\", \" Nat\", \"pet\", \" close\", \" plants\", \" for\", \" maintenance\", \"\\\\newline\", \"\\\\newline\", \"3\", \":\", \"38\", \" AM\", \" M\", \"ST\", \" |\", \" January\", \" 28\", \",\", \" 2013\", \" |\", \" Nat\", \"asha\", \" Al\", \"per\", \"ow\", \"icz\", \"\\\\newline\", \"\\\\newline\", \"S\", \"audi\", \" Arabian\", \" Mining\", \" Company\", \"\\u2019\", \"s\", \" (\", \"\\n\", \"han\", \"\\u2013\", \"Y\", \"ich\", \"ang\", \" railway\", \"\\\\newline\", \"\\\\newline\", \"H\", \"ank\", \"ou\", \"\\u2013\", \"Y\", \"ich\", \"ang\", \" railway\", \" (),\", \" or\", \" H\", \"any\", \"i\", \" railway\", \",\", \" is\", \" a\", \"  \", \"long\", \" high\", \"-\", \"speed\", \" railway\", \" between\", \" Hank\", \"ou\", \" (\", \"a\", \" borough\", \" of\", \" Wu\", \"\\n\", \"ator\", \" Elizabeth\", \" Warren\", \" at\", \" Boston\", \" Pride\", \" 2018\", \".\", \" (\", \"Photo\", \" by\", \" Alex\", \" Wong\", \"/\", \"Getty\", \" Images\", \")\", \"\\\\newline\", \"\\\\newline\", \"Sen\", \"ator\", \" Elizabeth\", \" Warren\", \" has\", \" submitted\", \" a\", \" bill\", \" that\", \" would\", \" require\", \" the\", \" US\", \" government\", \" to\", \" refund\", \" gay\", \" couples\", \" the\", \" cost\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.2660940885543823e-05]], [[9.5367431640625e-07]], [[0.0]], [[4.76837158203125e-07]], [[0.0]], [[7.152557373046875e-06]], [[0.0]], [[0.0]], [[1.6689300537109375e-06]], [[0.0]], [[0.0]], [[-0.8244547843933105]], [[0.02663397789001465]], [[-0.02203369140625]], [[-0.0019172430038452148]], [[-0.009816169738769531]], [[-0.000965118408203125]], [[-0.10572195053100586]], [[-0.19019508361816406]], [[0.0029916763305664062]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.4721097946166992]], [[-0.0013878715690225363]], [[-0.016162395477294922]], [[-0.10900020599365234]], [[-0.021924495697021484]], [[0.022090911865234375]], [[-0.017979860305786133]], [[-0.007839679718017578]], [[-0.0062389373779296875]], [[0.021077632904052734]], [[-0.02123880386352539]], [[-0.0035533905029296875]], [[-0.08024859428405762]], [[-0.016449451446533203]], [[-0.0008088350296020508]], [[0.007182717323303223]], [[-0.008461236953735352]], [[-0.0007710456848144531]], [[0.006113409996032715]], [[0.019765377044677734]], [[-0.0013922452926635742]], [[-8.573569357395172e-05]], [[-0.0056040287017822266]], [[-0.026596546173095703]], [[-0.03097987174987793]], [[0.00016307830810546875]], [[0.010325193405151367]], [[-0.001521080732345581]], [[-0.0013834387063980103]], [[0.002818584442138672]], [[-0.00940704345703125]], [[-0.0025281906127929688]], [[0.0060051679611206055]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-1.2382242679595947]], [[-0.0640878677368164]], [[-0.02431201934814453]], [[-0.012892723083496094]], [[0.03975534439086914]], [[-0.06369590759277344]], [[0.0014414787292480469]], [[0.0009407997131347656]], [[-0.028794288635253906]], [[-0.0003381967544555664]], [[0.06326842308044434]], [[0.0252230167388916]], [[-0.00030750036239624023]], [[-0.003420114517211914]], [[0.006350040435791016]], [[-0.013168811798095703]], [[0.0004534721374511719]], [[0.012208223342895508]], [[-0.04745173454284668]], [[-0.07330036163330078]], [[-0.03153562545776367]], [[-0.006133019924163818]], [[-0.009708523750305176]], [[-0.026589393615722656]], [[-0.0016218572854995728]], [[-0.0006702244281768799]], [[0.06024456024169922]], [[-0.006286144256591797]], [[-0.031679630279541016]], [[0.0037865638732910156]], [[-0.004018247127532959]], [[0.048207759857177734]], [[0.004985332489013672]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.4465198516845703]], [[-0.5868024826049805]], [[-0.8082046508789062]], [[-0.13875532150268555]], [[0.08264636993408203]], [[0.09876728057861328]], [[0.0009807348251342773]], [[-0.051762938499450684]], [[-0.06420278549194336]], [[-0.02983573079109192]], [[0.0002846717834472656]], [[-0.013984501361846924]], [[0.008867740631103516]], [[-0.019808530807495117]], [[-0.06805944442749023]], [[-0.003995746374130249]], [[0.013442039489746094]], [[0.04284477233886719]], [[-0.01129603385925293]], [[0.005185127258300781]], [[0.01025843620300293]], [[0.06460189819335938]], [[0.011432170867919922]], [[0.0008459091186523438]], [[-0.010104179382324219]], [[-0.026857852935791016]], [[0.0]], [[0.0]], [[0.0]], [[-0.2902059555053711]], [[-0.47748470306396484]], [[-0.4359172582626343]], [[-0.20672869682312012]], [[-0.0318608283996582]], [[-0.09755992889404297]], [[-0.02242213487625122]], [[-0.027380943298339844]], [[0.013363003730773926]], [[-0.05783069133758545]], [[-0.0012739300727844238]], [[-0.0009369850158691406]], [[-0.024805188179016113]], [[-0.008476853370666504]], [[0.03125753998756409]], [[0.0019244765862822533]], [[0.0032622870057821274]], [[0.03659975528717041]], [[-0.05031013488769531]], [[-0.00406646728515625]], [[0.020746946334838867]], [[-0.005303859710693359]], [[-0.003231912851333618]], [[-0.004206657409667969]], [[-0.0024569034576416016]], [[-0.0006527900695800781]], [[0.00013522803783416748]], [[0.0011617839336395264]], [[-0.04347562789916992]], [[0.0017123222351074219]], [[-0.01943337917327881]], [[0.009477615356445312]], [[0.00037473440170288086]], [[-0.006381690502166748]], [[-0.00045055150985717773]], [[-0.0031251907348632812]], [[-0.03169536590576172]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.57763671875e-05]], [[0.023873090744018555]], [[0.031331539154052734]], [[0.009867668151855469]], [[-0.03918123245239258]], [[0.009128332138061523]], [[0.05542182922363281]], [[-0.029610395431518555]], [[0.06895288825035095]], [[-0.011410236358642578]], [[0.02575099468231201]], [[0.029199600219726562]], [[-0.021152496337890625]], [[-0.0030771493911743164]], [[0.014663219451904297]], [[0.008498430252075195]], [[0.020249664783477783]], [[0.01597142219543457]], [[0.00029335450381040573]], [[0.014758586883544922]], [[0.0006477534770965576]], [[0.0003222227096557617]], [[0.020047903060913086]], [[-0.04209482669830322]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.0545196533203125]], [[0.0023374557495117188]], [[-8.720159530639648e-05]], [[-0.0003209114074707031]], [[0.0006334781646728516]], [[-0.0012798309326171875]], [[7.152557373046875e-06]], [[-0.0003390312194824219]], [[0.00043582916259765625]], [[-0.0003447532653808594]], [[-0.0004260540008544922]], [[-0.0005464553833007812]], [[-0.0005903244018554688]], [[-0.00017714500427246094]], [[0.0006191730499267578]], [[0.0016012191772460938]], [[-0.0003044605255126953]], [[-9.775161743164062e-05]], [[0.00036716461181640625]], [[1.430511474609375e-05]], [[-0.00012969970703125]], [[-9.059906005859375e-06]], [[0.0015668869018554688]], [[-0.00031280517578125]], [[-0.0004911422729492188]], [[0.06459379196166992]], [[-0.004400521516799927]], [[0.08646214008331299]], [[0.2748546600341797]], [[-0.002445220947265625]], [[-0.000980973243713379]], [[0.0005931854248046875]], [[-0.001468658447265625]], [[-0.00015713274478912354]], [[0.0018742084503173828]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2809562683105469]], [[0.06894397735595703]], [[-0.013441801071166992]], [[0.003071308135986328]], [[0.0005450248718261719]], [[-0.006158351898193359]], [[0.006029605865478516]], [[-0.01642155647277832]], [[0.007411956787109375]], [[0.0031824111938476562]], [[0.0003428459167480469]], [[0.004450619220733643]], [[0.007235050201416016]], [[-0.00014771893620491028]], [[0.02242361009120941]], [[0.0010271649807691574]], [[0.0011182967573404312]], [[-0.0216522216796875]], [[0.017052650451660156]], [[-0.01192939281463623]], [[-0.05205106735229492]], [[-0.018736958503723145]], [[-0.0039386749267578125]], [[0.006194114685058594]], [[0.0020537376403808594]], [[-0.0009042024612426758]], [[0.0005863606929779053]], [[0.005272865295410156]], [[-0.0005080848932266235]], [[-3.5692937672138214e-07]], [[0.0]], [[0.0]], [[-0.08371162414550781]], [[-0.10255050659179688]], [[0.005435943603515625]], [[0.02409839630126953]], [[-0.02150726318359375]], [[0.009768486022949219]], [[-0.005196094512939453]], [[0.004855155944824219]], [[0.0009253025054931641]], [[0.000330507755279541]], [[-0.00037670135498046875]], [[-0.007781982421875]], [[-0.010730266571044922]], [[-0.012881994247436523]], [[0.003911495208740234]], [[-0.013180255889892578]], [[-0.00788736343383789]], [[-0.003975391387939453]], [[-0.0002512931823730469]], [[-0.0020097196102142334]], [[-0.0006716251373291016]], [[-0.005305886268615723]], [[-0.029067039489746094]], [[-0.0113372802734375]], [[-0.004794597625732422]], [[0.001056671142578125]], [[0.0007905960083007812]], [[0.007075905799865723]], [[0.0010124444961547852]], [[-7.342547178268433e-06]], [[-0.0015921592712402344]], [[-0.009664058685302734]], [[-0.004057407379150391]], [[0.004569053649902344]], [[-0.0001379251480102539]], [[-0.005978107452392578]], [[-3.2221898436546326e-05]], [[-0.0020661354064941406]], [[0.0]], [[4.76837158203125e-07]], [[5.054473876953125e-05]], [[3.314018249511719e-05]], [[5.0067901611328125e-05]], [[0.00016546249389648438]], [[0.00046539306640625]], [[-3.266334533691406e-05]], [[3.606081008911133e-05]], [[0.000186920166015625]], [[1.621246337890625e-05]], [[-3.647804260253906e-05]], [[-8.928775787353516e-05]], [[-0.0001379251480102539]], [[-7.659196853637695e-06]], [[4.761386662721634e-07]], [[4.6193599700927734e-07]], [[0.0002894401550292969]], [[-0.00014352798461914062]], [[0.00012803077697753906]], [[0.00024271011352539062]], [[0.03818225860595703]], [[0.01227259635925293]], [[0.0012049674987792969]], [[0.00032901763916015625]], [[6.0439109802246094e-05]], [[-0.0013861656188964844]], [[-0.00048291683197021484]], [[0.0003857612609863281]], [[0.00010724365711212158]], [[0.00018984079360961914]], [[-4.190206527709961e-05]], [[0.00038242340087890625]], [[-0.0038678646087646484]], [[-1.2191012501716614e-05]], [[0.0006029605865478516]], [[0.0002384185791015625]], [[-0.0014772415161132812]], [[3.0100345611572266e-05]], [[0.0003724098205566406]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f7d2e9f9210>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_feature_direction_display(full_text, best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ports', ' coff', ' manufact', ' quarters', ' revenues', ' commerce', ' revenue', ' =>', ' tires', ' appealed', ' quarter', ' nombre', ' addressed', 'ftware', ' parity', ' port', ' Rab', ' notwithstanding', ' tract', ' humour']\n",
      "tensor([1.0664, 1.0092, 0.8916, 0.8852, 0.8766, 0.8653, 0.8618, 0.8567, 0.8510,\n",
      "        0.8441, 0.8361, 0.8341, 0.8334, 0.8325, 0.8312, 0.8150, 0.8143, 0.8104,\n",
      "        0.8097, 0.8046])\n"
     ]
    }
   ],
   "source": [
    "logit_lens(model,best_feature, smaller_dict, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_text = [\n",
    "    \"I can count up to: 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384 32768 7 6 12 16 18 20 22 24\",\n",
    "]\n",
    "visualize_text(custom_text, best_feature, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Centric Viewpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through datapoints & see if the features that activate on them make sense.\n",
    "d_point = 0\n",
    "# text = tokens_dataset[d_point]\n",
    "data_ind, sequence_pos = np.unravel_index(d_point, (datapoints, token_amount))\n",
    "feature_val, feature_ind = dictionary_activations[d_point].topk(10)\n",
    "data_ind = int(data_ind)\n",
    "sequence_pos = int(sequence_pos)\n",
    "full_tok = torch.tensor(dataset[data_ind][\"input_ids\"])\n",
    "full_text = []\n",
    "full_text.append(model.tokenizer.decode(full_tok))\n",
    "visualize_text(full_text, feature_ind, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the neuron/residual basis\n",
    "When we look at the weights of a feature, we are seeing the literal dimensions from the residual stream/neurons being read from the feature. \n",
    "\n",
    "Here I'm visualizing the weight values for the residual stream. If there are outliers, then it's mainly reading from that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights*max_activation).topk(20), (weights*max_activation).topk(20, largest=False).values, (weights*max_activation > 0.2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepend/Append tokens\n",
    "We can iterate over all tokens to check which ones activate a feature a lot to more rigorously test a hypothesis on what a feature means.\n",
    "\n",
    "Note: I'm literately running the model through all 50k tokens prepended to the text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_all_tokens_and_get_feature_activation(model, minimal_activating_example, feature, setting=\"prepend\"):\n",
    "    tokens = model.to_tokens(minimal_activating_example, prepend_bos=False)\n",
    "\n",
    "    # Run through every number up to vocab size\n",
    "    vocab_size = model.cfg.d_vocab\n",
    "    batch_size = 256*2 # Define your desired batch size\n",
    "\n",
    "    dollar_feature_activations = torch.zeros(vocab_size)\n",
    "    for start in range(0, vocab_size, batch_size):\n",
    "        end = min(start + batch_size, vocab_size)\n",
    "\n",
    "        token_prep = torch.arange(start, end).to(device)\n",
    "        token_prep = token_prep.unsqueeze(1)  # Add a dimension for concatenation\n",
    "\n",
    "        # 1. Prepend to the tokens\n",
    "        if setting == \"prepend\":\n",
    "            tokens_catted = torch.cat((token_prep, tokens.repeat(end - start, 1)), dim=1).long()\n",
    "        elif setting == \"append\":\n",
    "            tokens_catted = torch.cat((tokens.repeat(end - start, 1), token_prep), dim=1).long()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown setting: {setting}\")\n",
    "\n",
    "        # 2. Run through the model\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_catted.to(device))\n",
    "            neuron_act_batch = cache[cache_name]\n",
    "            _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "\n",
    "        # 3. Get the feature\n",
    "        dollar_feature_activations[start:end] = act[:, -1, feature].cpu().squeeze()\n",
    "\n",
    "    k = 20\n",
    "    k_increasing_val, k_increasing_ind = dollar_feature_activations.topk(k)\n",
    "    k_decreasing_val, k_decreasing_ind = dollar_feature_activations.topk(k, largest=False)\n",
    "    if(setting == \"prepend\"):\n",
    "        print(f\"[token]{minimal_activating_example}\")\n",
    "    elif(setting == \"append\"):\n",
    "        print(f\"{minimal_activating_example}[token]\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown setting: {setting}\")\n",
    "    # Print indices converted to tokens\n",
    "    print(f\"Top-{k} increasing: {model.to_str_tokens(k_increasing_ind)}\")\n",
    "    # Print values\n",
    "    print(f\"Top-{k} increasing: {[f'{val:.2f}' for val in k_increasing_val]}\")\n",
    "    print(f\"Top-{k} decreasing: {model.to_str_tokens(k_decreasing_ind)}\")\n",
    "    print(f\"Top-{k} decreasing: {[f'{val:.2f}' for val in k_decreasing_val]}\")\n",
    "    print(f\"Number of 0 activations: {torch.sum(dollar_feature_activations == 0)}\")\n",
    "    if(setting == \"prepend\"):\n",
    "        best_text = \"\".join(model.to_str_tokens(dollar_feature_activations.argmax()) + [minimal_activating_example])\n",
    "    else:\n",
    "        best_text = \"\".join([minimal_activating_example] + model.to_str_tokens(dollar_feature_activations.argmax()))\n",
    "    return best_text\n",
    "\n",
    "best_text = \"\"\n",
    "for x in range(3):\n",
    "    # best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" for all $\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_text = \"\"\n",
    "for x in range(3):\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepend_all_tokens_and_get_feature_activation(model, \" for all $\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \"The\", best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
